{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 3 - Support Vector Machines\n",
    "\n",
    "A SVM classifier builds a set of hyper-planes to try and separate the data by maximizing the distance between the borders and the data points.\n",
    "\n",
    "![SVM](http://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_0011.png \"Decision border in an SVM\")\n",
    "\n",
    "This separation is generally not possible to achieve in the original data space. Therefore, the first step of the SVM is to project the data into a high or infinite dimensions space in which this linear separation can be done. The projection can be done with linear, polynomial, or more comonly \"RBF\" kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading training data\n",
      "Pre-loading test data\n"
     ]
    }
   ],
   "source": [
    "from lab_tools import CIFAR10, evaluate_classifier, get_hog_image\n",
    "\n",
    "dataset = CIFAR10('./CIFAR10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a simple SVM** using [the SVC (Support Vector Classfiication) from sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). \n",
    "**Train** it on the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "SVM Classifier Predictive Performance (Accuracy) on Test Data: 0.8323333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lab_tools import CIFAR10\n",
    "\n",
    "# Normalize the HOG features\n",
    "scaler = StandardScaler()\n",
    "# Normalize the HOG features\n",
    "scaled_train_hog = scaler.fit_transform(dataset.train['hog'])\n",
    "scaled_test_hog = scaler.transform(dataset.test['hog'])\n",
    "\n",
    "# Initialize SVC (Support Vector Classification)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "# Define the parameter grid for the SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize StratifiedKFold with 5 folds\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the classifier, parameter grid, StratifiedKFold, and scoring\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train the grid search to find the best combination of hyperparameters\n",
    "grid_search.fit(scaled_train_hog, dataset.train['labels'])\n",
    "\n",
    "# Get the best estimator (classifier) from the grid search\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "\n",
    "# Now, you can use this best classifier to make predictions on the test set\n",
    "svm_test_preds = best_svm_clf.predict(scaled_test_hog)\n",
    "svm_test_accuracy = accuracy_score(dataset.test['labels'], svm_test_preds)\n",
    "\n",
    "# Print the best parameters and test accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"SVM Classifier Predictive Performance (Accuracy) on Test Data:\", svm_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251333333333333\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the classifier**. How many support vectors are there? What are support vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_support_vectors = clf.support_vectors_ #Each line = 1 \"Support Vector\" ; 1024 columns forming a 32x32 image \n",
    "vectors_per_class = clf.n_support_ #Number of \"Support Vector\" for each class\n",
    "\n",
    "# -- Your code here -- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to find the best \"C\" (error penalty) and \"gamma\" parameters** using cross-validation. What influence does \"C\" have on the number of support vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -- Your code here -- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing algorithms\n",
    "\n",
    "Using the best hyper-parameters that you found for each of the algorithms (kNN, Decision Trees, Random Forests, MLP, SVM):\n",
    "\n",
    "* Re-train the models on the full training set.\n",
    "* Compare their results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -- Your code here -- #\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
